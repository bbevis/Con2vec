{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "488d4628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bb320/Library/CloudStorage/GoogleDrive-burint@bnmanalytics.com/My Drive/Imperial/01_Projects/TeamofRivals/Analysis/Con2vec-1\n",
      "/Users/bb320/Library/CloudStorage/GoogleDrive-burint@bnmanalytics.com/My Drive/Imperial/01_Projects/TeamofRivals/Analysis/Con2vec-1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "print(os.getcwd())\n",
    "# os.chdir('../')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6fa910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Output/super_May22/mm_data_agg.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa584af",
   "metadata": {},
   "source": [
    "Generate labels of important turns from Transformer model using attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d6c658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TransformerEncoder with padding masks isn't fully supported. \n",
    "# torch.nn.TransformerEncoder tries to use an operator (_nested_tensor_from_mask_left_aligned) that hasnâ€™t been implemented for MPS yet.\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "os.environ[\"PYTORCH_DISABLE_MPS_FALLBACK\"] = \"1\"  # override fallback misbehavior\n",
    "\n",
    "torch.backends.mps.is_available = lambda: False  # Completely block MPS from being used\n",
    "\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 1. Data Preparation\n",
    "#############################################\n",
    "def prepare_data(df, feature_cols, label_map):\n",
    "    \"\"\"\n",
    "    Prepares padded turn sequences, attention masks, and labels from the raw DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe with PairID, Turn, features, and Negotiation_Category.\n",
    "        feature_cols (list): List of feature column names.\n",
    "        label_map (dict): Mapping of negotiation categories to integers.\n",
    "\n",
    "    Returns:\n",
    "        X_padded (Tensor): [B, T, F] padded feature sequences.\n",
    "        attention_mask (Tensor): [B, T] mask (1 = real, 0 = padding).\n",
    "        y_tensor (Tensor): [B] conversation-level labels.\n",
    "        pair_ids (list): List of PairIDs in the batch order.\n",
    "    \"\"\"\n",
    "    df['NegotiationOutcomeLabel'] = df['Negotiation_Category'].map(label_map)\n",
    "\n",
    "    grouped = df.groupby('PairID')\n",
    "\n",
    "    X_list, y_list, pair_ids = [], [], []\n",
    "\n",
    "    for pair_id, group in grouped:\n",
    "        group = group.sort_values('Turn')\n",
    "        features = torch.tensor(group[feature_cols].values, dtype=torch.float32)\n",
    "        label = torch.tensor([group['NegotiationOutcomeLabel'].iloc[0]], dtype=torch.long)\n",
    "        X_list.append(features)\n",
    "        y_list.append(label)\n",
    "        pair_ids.append(pair_id)\n",
    "\n",
    "    X_padded = pad_sequence(X_list, batch_first=True)\n",
    "    attention_mask = torch.zeros(X_padded.shape[:2], dtype=torch.bool)\n",
    "    for i, seq in enumerate(X_list):\n",
    "        attention_mask[i, :seq.shape[0]] = 1\n",
    "\n",
    "    y_tensor = torch.cat(y_list)\n",
    "    return X_padded, attention_mask, y_tensor, pair_ids\n",
    "\n",
    "#############################################\n",
    "# 2. Transformer Model Definition\n",
    "# This class defines a transformer-based model for classifying conversations.\n",
    "# Each input is a sequence of turns, where each turn is represented by behavioral features (e.g., facial, vocal, linguistic cues).\n",
    "# The model uses self-attention to learn which turns are most influential in predicting the outcome (e.g., Constructive, Destructive, etc.).\n",
    "#############################################\n",
    "class TurnTransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim=64, num_heads=4, num_layers=2, num_classes=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=128,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.classifier = nn.Linear(model_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x: input tensor of shape [batch_size, num_turns, num_features]\n",
    "        # mask: binary tensor [batch_size, num_turns], where 1 = real turn, 0 = padded\n",
    "        # This function returns:\n",
    "        # - logits: predicted class scores for each conversation\n",
    "        # - x: the internal transformer outputs per turn, used for attention-based interpretation\n",
    "        x = self.embedding(x)\n",
    "        transformer_mask = ~mask if mask is not None else None\n",
    "        x = self.transformer(x, src_key_padding_mask=transformer_mask)\n",
    "\n",
    "        if mask is not None:\n",
    "            lengths = mask.sum(dim=1).unsqueeze(1)\n",
    "            pooled = (x * mask.unsqueeze(-1)).sum(dim=1) / lengths\n",
    "        else:\n",
    "            pooled = x.mean(dim=1)\n",
    "\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits, x  # Return both logits and transformer output for attention analysis\n",
    "\n",
    "#############################################\n",
    "# 3. Utilities\n",
    "#############################################\n",
    "def get_label_map():\n",
    "    return {\n",
    "        'Constructive': 0,\n",
    "        'Destructive': 1,\n",
    "        'Friendly': 2,\n",
    "        'Apathetic': 3\n",
    "    }\n",
    "\n",
    "def get_feature_columns(df):\n",
    "    # Explicitly define the behavioral feature columns from Acknowledgement to Smile\n",
    "    start_col = 'Acknowledgement'\n",
    "    end_col = 'Smile'\n",
    "    cols = df.columns.tolist()\n",
    "    start_idx = cols.index(start_col)\n",
    "    end_idx = cols.index(end_col) + 1  # +1 because slicing is exclusive\n",
    "    return cols[start_idx:end_idx]\n",
    "\n",
    "#############################################\n",
    "# 4. Training Loop\n",
    "#############################################\n",
    "def train_model(model, X, mask, y, num_epochs=20, batch_size=8, lr=1e-3):\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "    # macOS does not support CUDA (NVIDIA GPUs), but if using a compatible system, MPS (Metal Performance Shaders) can accelerate training\n",
    "    # This block assumes MPS; skip DataParallel which is CUDA-dependent\n",
    "\n",
    "    model.to(device)\n",
    "    dataset = TensorDataset(X, mask, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss, total_correct = 0.0, 0\n",
    "        for xb, mb, yb in dataloader:\n",
    "            xb, mb, yb = xb.to(device), mb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = model(xb, mb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            total_correct += (preds == yb).sum().item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader.dataset)\n",
    "        accuracy = total_correct / len(dataloader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f} - Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "#############################################\n",
    "# 5. Attention Extraction and Influence Labeling\n",
    "# This function helps us identify which turns the transformer considers important.\n",
    "# It uses the L2 norm (magnitude) of each turn's final transformer output as a proxy for importance.\n",
    "# We flag the top X% of these high-importance turns as 'influential'.\n",
    "#############################################\n",
    "def extract_attention_scores(transformer_output, attention_mask, top_pct=0.5):\n",
    "    \"\"\"\n",
    "    Computes mean L2 norm of transformer outputs per turn and flags top X% as influential.\n",
    "\n",
    "    Args:\n",
    "        transformer_output (Tensor): [B, T, D] transformer output vectors\n",
    "        attention_mask (Tensor): [B, T] mask (1 = valid, 0 = padded)\n",
    "        top_pct (float): Proportion of top turns to flag as influential\n",
    "\n",
    "    Returns:\n",
    "        influence_matrix (Tensor): [B, T] binary matrix (1 = influential)\n",
    "        scores (Tensor): [B, T] importance scores (normalized per conversation)\n",
    "    \"\"\"\n",
    "    norms = transformer_output.norm(dim=-1)  # [B, T]\n",
    "    norms = norms * attention_mask  # zero out padded turns\n",
    "\n",
    "    influence_matrix = torch.zeros_like(norms)\n",
    "    scores = torch.zeros_like(norms)\n",
    "\n",
    "    for i in range(norms.shape[0]):\n",
    "        valid_scores = norms[i][attention_mask[i]]\n",
    "        if len(valid_scores) == 0:\n",
    "            continue\n",
    "        threshold = torch.quantile(valid_scores, 1 - top_pct)\n",
    "        influence_matrix[i] = (norms[i] >= threshold).int()\n",
    "        scores[i] = norms[i] / valid_scores.max()\n",
    "\n",
    "    return influence_matrix, scores\n",
    "\n",
    "#############################################\n",
    "# 6. Inference Helper\n",
    "# This function runs the model in evaluation mode on MPS (Apple GPU) or CPU,\n",
    "# returning the transformer outputs used for influence analysis.\n",
    "def run_model_inference(model, X, mask):\n",
    "    \"\"\"\n",
    "    Runs the trained transformer model in evaluation mode and returns transformer outputs.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained TurnTransformerClassifier\n",
    "        X (Tensor): Padded input tensor [B, T, F]\n",
    "        mask (Tensor): Attention mask [B, T] (1 = real, 0 = padding)\n",
    "\n",
    "    Returns:\n",
    "        transformer_output (Tensor): Transformer output for each turn [B, T, D]\n",
    "    \"\"\"\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    X = X.to(device)\n",
    "    mask = mask.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, transformer_output = model(X, mask)\n",
    "    return transformer_output\n",
    "\n",
    "#############################################\n",
    "# 7. Influence Output DataFrame\n",
    "# This function creates a new DataFrame by tagging each turn as influential (1) or not (0),\n",
    "# based on the transformer's internal output norms.\n",
    "def create_influence_dataframe(df, pair_ids, influence_matrix):\n",
    "    \"\"\"\n",
    "    Adds an 'Influential' column to the original dataframe using attention scores.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Original dataframe with PairID and Turn.\n",
    "        pair_ids (list): List of PairIDs matching the model input batch order.\n",
    "        influence_matrix (Tensor): [B, T] binary tensor of influential turns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Original dataframe with an added 'Influential' column (0 or 1).\n",
    "    \"\"\"\n",
    "    influence_records = []\n",
    "\n",
    "    for i, pair_id in enumerate(pair_ids):\n",
    "        convo_df = df[df['PairID'] == pair_id].sort_values('Turn').reset_index(drop=True)\n",
    "        influence_flags = influence_matrix[i][:len(convo_df)].cpu().numpy()\n",
    "        convo_df['Influential'] = influence_flags\n",
    "        influence_records.append(convo_df)\n",
    "\n",
    "    return pd.concat(influence_records, ignore_index=True)\n",
    "\n",
    "\n",
    "# NOTE: The behavioral features are the turn-level inputs from your dataset,\n",
    "# passed in via `feature_cols` during `prepare_data()`. These get embedded\n",
    "# and processed by the transformer in sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b42437f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Loss: 1.4189 - Accuracy: 0.1395\n",
      "Epoch 2/20 - Loss: 1.4263 - Accuracy: 0.3023\n",
      "Epoch 3/20 - Loss: 1.3751 - Accuracy: 0.2791\n",
      "Epoch 4/20 - Loss: 1.3749 - Accuracy: 0.3256\n",
      "Epoch 5/20 - Loss: 1.3198 - Accuracy: 0.3488\n",
      "Epoch 6/20 - Loss: 1.3216 - Accuracy: 0.3256\n",
      "Epoch 7/20 - Loss: 1.3089 - Accuracy: 0.3721\n",
      "Epoch 8/20 - Loss: 1.2843 - Accuracy: 0.3721\n",
      "Epoch 9/20 - Loss: 1.2744 - Accuracy: 0.4186\n",
      "Epoch 10/20 - Loss: 1.2181 - Accuracy: 0.4186\n",
      "Epoch 11/20 - Loss: 1.2163 - Accuracy: 0.3953\n",
      "Epoch 12/20 - Loss: 1.1747 - Accuracy: 0.4186\n",
      "Epoch 13/20 - Loss: 1.2241 - Accuracy: 0.4419\n",
      "Epoch 14/20 - Loss: 1.3349 - Accuracy: 0.3256\n",
      "Epoch 15/20 - Loss: 1.1550 - Accuracy: 0.4651\n",
      "Epoch 16/20 - Loss: 1.2567 - Accuracy: 0.3488\n",
      "Epoch 17/20 - Loss: 1.1285 - Accuracy: 0.4884\n",
      "Epoch 18/20 - Loss: 1.0921 - Accuracy: 0.5116\n",
      "Epoch 19/20 - Loss: 1.0493 - Accuracy: 0.4651\n",
      "Epoch 20/20 - Loss: 1.1372 - Accuracy: 0.5349\n",
      "âœ… Done. Output saved to conversation_with_influence.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Identify behavioral feature columns\n",
    "feature_cols = get_feature_columns(df)\n",
    "\n",
    "# Define label mapping\n",
    "label_map = get_label_map()\n",
    "\n",
    "# Prepare model inputs (turn-level tensors, attention masks, labels)\n",
    "X_padded, attention_mask, y_tensor, pair_ids = prepare_data(df, feature_cols, label_map)\n",
    "\n",
    "# Initialize model\n",
    "model = TurnTransformerClassifier(input_dim=len(feature_cols))\n",
    "\n",
    "# Train model (can be skipped if model is pre-trained)\n",
    "train_model(model, X_padded, attention_mask, y_tensor)\n",
    "\n",
    "# Run inference to get per-turn transformer outputs\n",
    "transformer_output = run_model_inference(model, X_padded, attention_mask)\n",
    "\n",
    "# Extract influence scores and flags (e.g., top 20% of turns)\n",
    "influence_matrix, scores = extract_attention_scores(transformer_output, attention_mask, top_pct=0.2)\n",
    "\n",
    "# Create updated DataFrame with \"Influential\" column\n",
    "df_with_influence = create_influence_dataframe(df, pair_ids, influence_matrix)\n",
    "\n",
    "# Save updated dataset\n",
    "df_with_influence.to_csv(\"./Output/super_May22/conversation_with_influence.csv\", index=False)\n",
    "print(\"âœ… Done. Output saved to conversation_with_influence.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "641a9c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Pair_Speaker_turn', 'PairID', 'PersonID', 'Speaker',\n",
       "       'Speaker_original', 'Turn', 'Word', 'StartTime', 'EndTime',\n",
       "       'Backchannel', 'Overlap', 'Contested', 'Duration',\n",
       "       'Negotiation_Category', 'Conflict', 'team_viability', 'Sentiment',\n",
       "       'word_count', 'Acknowledgement', 'Affirmation', 'Agreement', 'Apology',\n",
       "       'Ask_Agency', 'By_The_Way', 'Can_You', 'Conjunction_Start', 'Could_You',\n",
       "       'Disagreement', 'Filler_Pause', 'First_Person_Plural',\n",
       "       'First_Person_Single', 'For_Me', 'For_You', 'Formal_Title',\n",
       "       'Give_Agency', 'Goodbye', 'Gratitude', 'Hedges', 'Hello',\n",
       "       'Impersonal_Pronoun', 'Informal_Title', 'Let_Me_Know', 'Negation',\n",
       "       'Negative_Emotion', 'Please', 'Positive_Emotion', 'Reasoning',\n",
       "       'Reassurance', 'Second_Person', 'Subjectivity', 'Swearing',\n",
       "       'Truth_Intensifier', 'Bare_Command', 'YesNo_Questions', 'WH_Questions',\n",
       "       'Adverb_Limiter', 'Token_count', 'Pitch', 'Vocal Intensity',\n",
       "       'Vocal Articulation', 'Vocal Clarity', 'SquintPuffSneer',\n",
       "       'Compressed_Contortion', 'Brow_Up', 'Cheek_Puff_Sneer', 'Blink',\n",
       "       'Look_Down', 'Look_Right', 'Look_Left', 'Look_Up', 'Blink_Right',\n",
       "       'Eyes_Wide', 'Open_Mouth', 'Jaw_Left', 'Jaw_Right',\n",
       "       'mouthClose_openJaw', 'mouthDimple_Left', 'mouthDimple_Right',\n",
       "       'Skeptical_Right', 'Sneering_Pucker', 'mouth_Right', 'mouth_Roll',\n",
       "       'mouth_Shrug', 'Smile', 'label', 'NegotiationOutcomeLabel'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d63859",
   "metadata": {},
   "source": [
    "Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e68b5b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_precontested_flags(df, contested_col='Contested'):\n",
    "    \"\"\"\n",
    "    Adds 'PreContested' and 'Other' binary columns to the DataFrame.\n",
    "    - PreContested: 1 if the turn is immediately before a contested turn (same PairID), else 0\n",
    "    - Other: 1 if the turn is neither contested nor pre-contested, else 0\n",
    "    \"\"\"\n",
    "    df = df.sort_values(by=['PairID', 'Turn']).reset_index(drop=True)\n",
    "    df['PreContested'] = 0\n",
    "\n",
    "    contested_idx = df[df[contested_col] == 1].index\n",
    "\n",
    "    for idx in contested_idx:\n",
    "        if idx > 0 and df.loc[idx, 'PairID'] == df.loc[idx - 1, 'PairID']:\n",
    "            if df.loc[idx - 1, contested_col] == 0:\n",
    "                df.loc[idx - 1, 'PreContested'] = 1\n",
    "\n",
    "    df['Other'] = ((df[contested_col] == 0) & (df['PreContested'] == 0)).astype(int)\n",
    "    return df\n",
    "\n",
    "def compute_binary_correlation_with_pvalues(df):\n",
    "    \"\"\"\n",
    "    Computes correlation matrix and p-values for binary indicators:\n",
    "    Contested, PreContested, Other, and Influential.\n",
    "\n",
    "    Returns:\n",
    "        corr_matrix (DataFrame): Pearson correlation coefficients.\n",
    "        pval_matrix (DataFrame): Corresponding p-values.\n",
    "    \"\"\"\n",
    "    cols = ['Contested', 'PreContested', 'Other', 'Influential']\n",
    "    corr_matrix = pd.DataFrame(index=cols, columns=cols, dtype=float)\n",
    "    pval_matrix = pd.DataFrame(index=cols, columns=cols, dtype=float)\n",
    "\n",
    "    for col1 in cols:\n",
    "        for col2 in cols:\n",
    "            corr, pval = pearsonr(df[col1], df[col2])\n",
    "            corr_matrix.loc[col1, col2] = corr\n",
    "            pval_matrix.loc[col1, col2] = pval\n",
    "\n",
    "    return corr_matrix, pval_matrix\n",
    "\n",
    "# Example usage:\n",
    "# df = add_precontested_flags(df)\n",
    "# corr_matrix, pval_matrix = compute_binary_correlation_with_pvalues(df)\n",
    "# print(\"Correlations:\\n\", corr_matrix.round(2))\n",
    "# print(\"\\nP-values:\\n\", pval_matrix.round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "12fa6351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlations:\n",
      "               Contested  PreContested  Other  Influential\n",
      "Contested          1.00         -0.31  -0.60         0.05\n",
      "PreContested      -0.31          1.00  -0.40         0.07\n",
      "Other             -0.60         -0.40   1.00        -0.03\n",
      "Influential        0.05          0.07  -0.03         1.00\n",
      "\n",
      "P-values:\n",
      "               Contested  PreContested   Other  Influential\n",
      "Contested        0.0000           0.0  0.0000       0.0005\n",
      "PreContested     0.0000           0.0  0.0000       0.0000\n",
      "Other            0.0000           0.0  0.0000       0.0306\n",
      "Influential      0.0005           0.0  0.0306       0.0000\n"
     ]
    }
   ],
   "source": [
    "df = add_precontested_flags(df_with_influence)\n",
    "corr_matrix, pval_matrix = compute_binary_correlation_with_pvalues(df)\n",
    "print(\"Correlations:\\n\", corr_matrix.round(2))\n",
    "print(\"\\nP-values:\\n\", pval_matrix.round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "465e7c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dynamic threshold based on # of Contested turns = 1485.2832906248887\n",
      "Dynamic threshold value = 7.9735\n",
      "\n",
      "Accuracy between Influential_dynamic and Contested:\n",
      "Correct matches: 2552\n",
      "Incorrect matches: 2188\n",
      "Accuracy = 0.538 (53.8%)\n",
      "\n",
      "Precision / Recall / F1:\n",
      "Precision = 0.364\n",
      "Recall    = 0.368\n",
      "F1-score  = 0.366\n"
     ]
    }
   ],
   "source": [
    "# Compute L2 norms again\n",
    "l2_norms = transformer_output.norm(dim=-1).cpu().numpy()  # [B, T]\n",
    "attention_mask_np = attention_mask.cpu().numpy()\n",
    "\n",
    "# Prepare list to collect L2 norms in the same order as df\n",
    "l2_norm_list = []\n",
    "\n",
    "# You already have pair_ids in the same order as X_padded\n",
    "# Loop through each conversation and append valid L2 norms per turn\n",
    "for i, pair_id in enumerate(pair_ids):\n",
    "    convo_mask = attention_mask_np[i]\n",
    "    convo_l2 = l2_norms[i]\n",
    "\n",
    "    # Only take turns where mask == 1\n",
    "    valid_l2 = convo_l2[convo_mask == 1]\n",
    "\n",
    "    # Append to list\n",
    "    l2_norm_list.extend(valid_l2.tolist())\n",
    "\n",
    "# Now l2_norm_list should have the same length as df\n",
    "assert len(l2_norm_list) == len(df), f\"Expected {len(df)} L2 norms, got {len(l2_norm_list)}\"\n",
    "\n",
    "# Add to dataframe\n",
    "df_scores = df.copy()\n",
    "df_scores['L2_norm'] = l2_norm_list\n",
    "\n",
    "\n",
    "# Step 2: Determine dynamic threshold\n",
    "total_contested = df['Contested'].sum()\n",
    "\n",
    "# Sort scores in descending order\n",
    "sorted_scores = df_scores['L2_norm'].sort_values(ascending=False)\n",
    "\n",
    "# Determine threshold\n",
    "dynamic_threshold = sorted_scores.iloc[int(total_contested) - 1] if total_contested > 0 else sorted_scores.max()\n",
    "\n",
    "\n",
    "# Step 3: Apply threshold to generate new Influential column\n",
    "df_scores['Influential_dynamic'] = (df_scores['L2_norm'] >= dynamic_threshold).astype(int)\n",
    "\n",
    "# Step 4: Run overlap analysis\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Accuracy\n",
    "num_correct = (df_scores['Contested'] == df_scores['Influential_dynamic']).sum()\n",
    "num_incorrect = (df_scores['Contested'] != df_scores['Influential_dynamic']).sum()\n",
    "total_turns = df_scores.shape[0]\n",
    "accuracy = num_correct / total_turns\n",
    "\n",
    "# Force to integer just in case\n",
    "y_true = df_scores['Contested'].astype(int)\n",
    "y_pred = df_scores['Influential_dynamic'].astype(int)\n",
    "\n",
    "# Precision / Recall / F1\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nDynamic threshold based on # of Contested turns = {total_contested}\")\n",
    "print(f\"Dynamic threshold value = {dynamic_threshold:.4f}\")\n",
    "\n",
    "print(\"\\nAccuracy between Influential_dynamic and Contested:\")\n",
    "print(f\"Correct matches: {num_correct}\")\n",
    "print(f\"Incorrect matches: {num_incorrect}\")\n",
    "print(f\"Accuracy = {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nPrecision / Recall / F1:\")\n",
    "print(f\"Precision = {precision:.3f}\")\n",
    "print(f\"Recall    = {recall:.3f}\")\n",
    "print(f\"F1-score  = {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe8af60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_imperial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
