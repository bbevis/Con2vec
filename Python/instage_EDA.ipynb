{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bb320/Library/CloudStorage/GoogleDrive-burint@bnmanalytics.com/My Drive/Imperial/01_Projects/TeamofRivals/Analysis/Con2vec-1\n",
      "/Users/bb320/Library/CloudStorage/GoogleDrive-burint@bnmanalytics.com/My Drive/Imperial/01_Projects/TeamofRivals/Analysis/Con2vec-1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(os.getcwd())\n",
    "# os.chdir('../')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pair_Speaker_turn', 'PairID', 'PersonID', 'Speaker', 'Speaker_original', 'Turn', 'Speaker_turn', 'Turn_Boundary', 'Turn Start', 'Turn End', 'PairID_text', 'PersonID_text', 'Speaker_text', 'Speaker_original_text', 'Turn_text', 'Word', 'Start Time', 'End Time', 'Backchannel', 'Overlap', 'Contested', 'Duration', 'Sentiment', 'word_count', 'PairID_vocal', 'PersonID_vocal', 'Turn Start_vocal', 'Turn End_vocal', 'Rms', 'Pitch', 'Pulse', 'ZCR', 'Spectral_Centroid', 'Spectral_Bandwidth', 'positive_bert', 'negative_bert', 'neutral_bert', 'info_exchange_zscore_chats', 'discrepancies_lexical_wordcount', 'hear_lexical_wordcount', 'home_lexical_wordcount', 'conjunction_lexical_wordcount', 'certainty_lexical_wordcount', 'inclusive_lexical_wordcount', 'bio_lexical_wordcount', 'achievement_lexical_wordcount', 'adverbs_lexical_wordcount', 'anxiety_lexical_wordcount', 'third_person_lexical_wordcount', 'negation_lexical_wordcount', 'swear_lexical_wordcount', 'death_lexical_wordcount', 'health_lexical_wordcount', 'see_lexical_wordcount', 'body_lexical_wordcount', 'family_lexical_wordcount', 'negative_affect_lexical_wordcount', 'quantifier_lexical_wordcount', 'positive_affect_lexical_wordcount', 'insight_lexical_wordcount', 'humans_lexical_wordcount', 'present_tense_lexical_wordcount', 'future_tense_lexical_wordcount', 'past_tense_lexical_wordcount', 'relative_lexical_wordcount', 'sexual_lexical_wordcount', 'inhibition_lexical_wordcount', 'sadness_lexical_wordcount', 'social_lexical_wordcount', 'indefinite_pronoun_lexical_wordcount', 'religion_lexical_wordcount', 'work_lexical_wordcount', 'money_lexical_wordcount', 'causation_lexical_wordcount', 'anger_lexical_wordcount', 'first_person_singular_lexical_wordcount', 'feel_lexical_wordcount', 'tentativeness_lexical_wordcount', 'exclusive_lexical_wordcount', 'verbs_lexical_wordcount', 'friends_lexical_wordcount', 'article_lexical_wordcount', 'argue_lexical_wordcount', 'auxiliary_verbs_lexical_wordcount', 'cognitive_mech_lexical_wordcount', 'preposition_lexical_wordcount', 'first_person_plural_lexical_wordcount', 'percept_lexical_wordcount', 'second_person_lexical_wordcount', 'positive_words_lexical_wordcount', 'first_person_lexical_wordcount', 'nltk_english_stopwords_lexical_wordcount', 'textblob_subjectivity', 'textblob_polarity', 'positivity_zscore_chats', 'positivity_zscore_conversation', 'please_politeness_convokit', 'please_start_politeness_convokit', 'hashedge_politeness_convokit', 'indirect_btw_politeness_convokit', 'hedges_politeness_convokit', 'factuality_politeness_convokit', 'deference_politeness_convokit', 'gratitude_politeness_convokit', 'apologizing_politeness_convokit', '1st_person_pl_politeness_convokit', '1st_person_politeness_convokit', '1st_person_start_politeness_convokit', '2nd_person_politeness_convokit', '2nd_person_start_politeness_convokit', 'indirect_greeting_politeness_convokit', 'direct_question_politeness_convokit', 'direct_start_politeness_convokit', 'haspositive_politeness_convokit', 'hasnegative_politeness_convokit', 'subjunctive_politeness_convokit', 'indicative_politeness_convokit', 'Acknowledgement_receptiveness_yeomans', 'Affirmation_receptiveness_yeomans', 'Agreement_receptiveness_yeomans', 'Apology_receptiveness_yeomans', 'Ask_Agency_receptiveness_yeomans', 'By_The_Way_receptiveness_yeomans', 'Can_You_receptiveness_yeomans', 'Conjunction_Start_receptiveness_yeomans', 'Could_You_receptiveness_yeomans', 'Disagreement_receptiveness_yeomans', 'Filler_Pause_receptiveness_yeomans', 'First_Person_Plural_receptiveness_yeomans', 'First_Person_Single_receptiveness_yeomans', 'For_Me_receptiveness_yeomans', 'For_You_receptiveness_yeomans', 'Formal_Title_receptiveness_yeomans', 'Give_Agency_receptiveness_yeomans', 'Goodbye_receptiveness_yeomans', 'Gratitude_receptiveness_yeomans', 'Hedges_receptiveness_yeomans', 'Hello_receptiveness_yeomans', 'Impersonal_Pronoun_receptiveness_yeomans', 'Informal_Title_receptiveness_yeomans', 'Let_Me_Know_receptiveness_yeomans', 'Negation_receptiveness_yeomans', 'Negative_Emotion_receptiveness_yeomans', 'Please_receptiveness_yeomans', 'Positive_Emotion_receptiveness_yeomans', 'Reasoning_receptiveness_yeomans', 'Reassurance_receptiveness_yeomans', 'Second_Person_receptiveness_yeomans', 'Subjectivity_receptiveness_yeomans', 'Swearing_receptiveness_yeomans', 'Truth_Intensifier_receptiveness_yeomans', 'Bare_Command_receptiveness_yeomans', 'YesNo_Questions_receptiveness_yeomans', 'WH_Questions_receptiveness_yeomans', 'Adverb_Limiter_receptiveness_yeomans']\n"
     ]
    }
   ],
   "source": [
    "# merged_df = pd.read_csv('./Output/super_May22/Merged_data.csv')\n",
    "# linfeat = pd.read_csv('./Output/super_May22/may_super_turn_level_bb.csv')\n",
    "# merged_df = pd.concat([merged_df, linfeat], axis = 1)\n",
    "# print(list(merged_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Subset Data to Relevant Variables\n",
    "def subset_columns(df, audio_df):\n",
    "    # Select columns containing specific keywords from the main dataset\n",
    "    relevant_columns = [\n",
    "        col for col in df.columns \n",
    "        if any(keyword in col for keyword in ['lexical_wordcount', 'convokit', 'receptiveness'])\n",
    "    ]\n",
    "    \n",
    "    # Rename columns to remove specific substrings in their names\n",
    "    rename_mapping = {\n",
    "        col: col.replace('_lexical_wordcount', '').replace('_politeness_convokit', '').replace('_receptiveness_yeomans', '')\n",
    "        for col in relevant_columns\n",
    "    }\n",
    "    \n",
    "    df = df.rename(columns=rename_mapping)\n",
    "    \n",
    "\n",
    "    # Include specific audio variables from the audio dataset\n",
    "    audio_variables = ['Rms', 'Pitch', 'Pulse', 'ZCR', 'Spectral_Centroid', 'Spectral_Bandwidth']\n",
    "    audio_variable_mapping = {\n",
    "        'Rms': 'Loudness_Energy',\n",
    "        'Pitch': 'Intonation_Patterns',\n",
    "        'Pulse': 'Rhythm_Strength',\n",
    "        'ZCR': 'Voiced_Unvoiced_Distinction',\n",
    "        'Spectral_Centroid': 'Spectral_Center',\n",
    "        'Spectral_Bandwidth': 'Spectral_Width'\n",
    "    }\n",
    "    \n",
    "    # Rename audio variables for better interpretability\n",
    "    audio_df = audio_df[audio_variables].rename(columns=audio_variable_mapping)\n",
    "\n",
    "    # Combine the two datasets\n",
    "    \n",
    "    combined_df = pd.concat([df[list(rename_mapping.values())], audio_df], axis=1)\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Preprocess Data\n",
    "def preprocess_data(df, variables_to_include):\n",
    "    scaler = StandardScaler()\n",
    "    numeric_data = df[variables_to_include]\n",
    "    scaled_data = scaler.fit_transform(numeric_data)\n",
    "    return scaled_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Perform PCA and Clustering on Entire Dataset\n",
    "def perform_pca_clustering(df, variables_to_include, n_clusters=5):\n",
    "    scaled_data = preprocess_data(df, variables_to_include)\n",
    "    \n",
    "    # PCA\n",
    "    pca = PCA()\n",
    "    pca_data = pca.fit_transform(scaled_data)\n",
    "    n_components = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.8) + 1\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_data_reduced = pca.fit_transform(scaled_data)\n",
    "    \n",
    "    # Clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(pca.components_.T)\n",
    "    \n",
    "    # Assign clusters to variables\n",
    "    clusters = kmeans.labels_\n",
    "    clustered_variables = pd.DataFrame({\n",
    "        'Variable': variables_to_include,\n",
    "        'Cluster': clusters\n",
    "    })\n",
    "    \n",
    "    # Evaluate clustering\n",
    "    silhouette_avg = silhouette_score(pca.components_.T, clusters)\n",
    "    print(f\"Silhouette Score for entire dataset: {silhouette_avg:.2f}\")\n",
    "    \n",
    "    return clustered_variables, silhouette_avg, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Rank Clusters by Variability Across StageLabels and Generate Intuitive Names\n",
    "def rank_clusters_by_stage_label(clustered_variables, df, stage_label_col):\n",
    "    cluster_summary = []\n",
    "\n",
    "    for cluster_id in clustered_variables['Cluster'].unique():\n",
    "        cluster_vars = clustered_variables[clustered_variables['Cluster'] == cluster_id]['Variable']\n",
    "\n",
    "        # Generate descriptive name for the cluster based on variable composition\n",
    "        cluster_name = \", \".join(cluster_vars.head(3))  # Use top 3 variables as representation\n",
    "        if len(cluster_vars) > 3:\n",
    "            cluster_name += \", ...\"  # Indicate more variables if applicable\n",
    "\n",
    "        # Calculate variability of each cluster across StageLabel groups\n",
    "        cluster_variability = cluster_vars.apply(\n",
    "            lambda var: df.groupby(stage_label_col)[var].var().mean()\n",
    "        ).mean()\n",
    "\n",
    "        cluster_summary.append({\n",
    "            'Cluster': cluster_id,\n",
    "            'ClusterName': cluster_name,\n",
    "            'MeanVariability': cluster_variability\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame and rank by MeanVariability\n",
    "    cluster_summary_df = pd.DataFrame(cluster_summary)\n",
    "    cluster_summary_df = cluster_summary_df.sort_values(by='MeanVariability', ascending=False)\n",
    "\n",
    "    # Create rankings for each StageLabel\n",
    "    stage_label_rankings = {}\n",
    "    for label, segment_df in df.groupby(stage_label_col):\n",
    "        stage_ranking = []\n",
    "        for _, row in cluster_summary_df.iterrows():\n",
    "            cluster_id = row['Cluster']\n",
    "            cluster_vars = clustered_variables[clustered_variables['Cluster'] == cluster_id]['Variable']\n",
    "            stage_variability = cluster_vars.apply(\n",
    "                lambda var: segment_df[var].var()\n",
    "            ).mean()\n",
    "            stage_ranking.append((row['ClusterName'], stage_variability))\n",
    "        stage_label_rankings[label] = sorted(stage_ranking, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return cluster_summary_df, stage_label_rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score for entire dataset: 0.05\n",
      "Cluster Summary:\n",
      "   Cluster                             ClusterName  MeanVariability\n",
      "0        0          discrepancies, hear, home, ...     36093.165781\n",
      "2        3  social, second_person, 2nd_person, ...         4.497509\n",
      "1        2                negative_affect, sadness         0.041216\n",
      "4        1                                 Goodbye         0.005371\n",
      "3        4                          please, Please         0.002854\n",
      "\n",
      "StageLabel Rankings:\n",
      "Stage Stage 1 Rankings:\n",
      "  Cluster: discrepancies, hear, home, ..., Variability: 47182.53256218929\n",
      "  Cluster: social, second_person, 2nd_person, ..., Variability: 4.566395516500259\n",
      "  Cluster: negative_affect, sadness, Variability: 0.019377261474004946\n",
      "  Cluster: Goodbye, Variability: 0.0\n",
      "  Cluster: please, Please, Variability: 0.0\n",
      "Stage Stage 2 Rankings:\n",
      "  Cluster: discrepancies, hear, home, ..., Variability: 40623.508791958164\n",
      "  Cluster: social, second_person, 2nd_person, ..., Variability: 4.458667821169473\n",
      "  Cluster: negative_affect, sadness, Variability: 0.02728858167373555\n",
      "  Cluster: Goodbye, Variability: 0.009350983528767178\n",
      "  Cluster: please, Please, Variability: 0.0038566997852429096\n",
      "Stage Stage 3 Rankings:\n",
      "  Cluster: discrepancies, hear, home, ..., Variability: 27921.143841040077\n",
      "  Cluster: social, second_person, 2nd_person, ..., Variability: 5.65314234968445\n",
      "  Cluster: negative_affect, sadness, Variability: 0.042594396830901285\n",
      "  Cluster: Goodbye, Variability: 0.006060405898574897\n",
      "  Cluster: please, Please, Variability: 0.0033760662589699705\n",
      "Stage Stage 4 Rankings:\n",
      "  Cluster: discrepancies, hear, home, ..., Variability: 28645.477930626246\n",
      "  Cluster: social, second_person, 2nd_person, ..., Variability: 3.311829270223009\n",
      "  Cluster: negative_affect, sadness, Variability: 0.07560255191597806\n",
      "  Cluster: Goodbye, Variability: 0.006074213168959465\n",
      "  Cluster: please, Please, Variability: 0.004183894237778387\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "merged_df = pd.read_csv('./Output/super_May22/may_super_turn_level_bb.csv')\n",
    "audio_df = pd.read_csv('./Output/super_May22/Vocal_agg.csv')\n",
    "\n",
    "\n",
    "# Step 0: Subset columns\n",
    "merged_df = subset_columns(merged_df, audio_df)\n",
    "\n",
    "# Define variables to include\n",
    "variables_to_include = merged_df.columns.tolist()\n",
    "\n",
    "# 'Merge' StageLabel into the main dataset\n",
    "# Load StageLabel from external file\n",
    "stages_df = pd.read_csv('./Output/super_May22/Segmented_Conversations_With_Conflicts.csv')\n",
    "stage_label_col = 'Stage'\n",
    "merged_df[stage_label_col] = stages_df[stage_label_col]\n",
    "\n",
    "# Perform PCA and clustering on the entire dataset\n",
    "clustered_variables, silhouette_avg, pca = perform_pca_clustering(merged_df, variables_to_include, n_clusters=5)\n",
    "\n",
    "# Rank clusters by variability across StageLabels\n",
    "cluster_summary, stage_label_rankings = rank_clusters_by_stage_label(clustered_variables, merged_df, stage_label_col)\n",
    "\n",
    "# Output results\n",
    "print(\"Cluster Summary:\")\n",
    "print(cluster_summary)\n",
    "\n",
    "print(\"\\nStageLabel Rankings:\")\n",
    "for stage, ranking in stage_label_rankings.items():\n",
    "    print(f\"Stage {stage} Rankings:\")\n",
    "    for cluster_name, variability in ranking:\n",
    "        print(f\"  Cluster: {cluster_name}, Variability: {variability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
