{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring similarities of DVs and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m==>\u001b[0m \u001b[1mAuto-updating Homebrew...\u001b[0m\n",
      "Adjust how often this is run with HOMEBREW_AUTO_UPDATE_SECS or disable with\n",
      "HOMEBREW_NO_AUTO_UPDATE. Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mAuto-updated Homebrew!\u001b[0m\n",
      "Updated 2 taps (homebrew/core and homebrew/cask).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mNew Formulae\u001b[0m\n",
      "acme.sh             hypopg              redocly-cli         ubi\n",
      "astroterm           icann-rdap          sdl3                umka-lang\n",
      "babelfish           lazysql             sdl3_image          xeyes\n",
      "behaviortree.cpp    ludusavi            terraform-cleaner   xlsclients\n",
      "catgirl             martin              tf-summarize        xprop\n",
      "chiko               precious            tfprovidercheck     xwininfo\n",
      "\u001b[34m==>\u001b[0m \u001b[1mNew Casks\u001b[0m\n",
      "dana-dex                                 ik-product-manager\n",
      "dockfix                                  imaging-edge-webcam\n",
      "flashspace                               jottacloud\n",
      "flowvision                               leader-key\n",
      "font-bitcount                            linearmouse@beta\n",
      "font-bitcount-prop-single-ink            muteme\n",
      "font-bitcount-single-ink                 startupfolder\n",
      "font-maple-mono-cn                       turbotax-2024\n",
      "font-maple-mono-nf-cn                    valhalla-freq-echo\n",
      "font-monomakh                            valhalla-space-modulator\n",
      "freelens                                 valhalla-supermassive\n",
      "gpt4all\n",
      "\n",
      "You have \u001b[1m12\u001b[0m outdated formulae installed.\n",
      "\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libomp/manifests/19.1.7\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[32m==>\u001b[0m \u001b[1mFetching \u001b[32mlibomp\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libomp/blobs/sha256:f80484105bc\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring libomp--19.1.7.arm64_sequoia.bottle.tar.gz\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mCaveats\u001b[0m\n",
      "libomp is keg-only, which means it was not symlinked into /opt/homebrew,\n",
      "because it can override GCC headers and result in broken builds.\n",
      "\n",
      "For compilers to find libomp you may need to set:\n",
      "  export LDFLAGS=\"-L/opt/homebrew/opt/libomp/lib\"\n",
      "  export CPPFLAGS=\"-I/opt/homebrew/opt/libomp/include\"\n",
      "\u001b[34m==>\u001b[0m \u001b[1mSummary\u001b[0m\n",
      "🍺  /opt/homebrew/Cellar/libomp/19.1.7: 9 files, 1.7MB\n",
      "\u001b[34m==>\u001b[0m \u001b[1mRunning `brew cleanup libomp`...\u001b[0m\n",
      "Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.\n",
      "Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n"
     ]
    }
   ],
   "source": [
    "# !brew install libomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bb320/Library/CloudStorage/GoogleDrive-burint@bnmanalytics.com/My Drive/Imperial/01_Projects/TeamofRivals/Analysis/Con2vec-1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, adjusted_rand_score\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.utils import resample\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial.distance import cdist\n",
    "from kneed import KneeLocator\n",
    "import shap\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "\n",
    "print(os.getcwd())\n",
    "# os.chdir('../')\n",
    "# print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_raw = pd.read_csv('wbl_05_22_super_sabbatical_processed.csv')\n",
    "\n",
    "df = df_raw[['groupId',\n",
    " 'playerId',\n",
    " 'PostDiscussionGeneral_raw_discussionDepth',\n",
    " 'PostDiscussionGeneral_raw_discussionDisagreement',\n",
    " 'PostDiscussionGeneral_raw_discussionEnjoy',\n",
    " 'PostDiscussionGeneral_raw_discussionTension',\n",
    " 'PostDiscussionGeneral_raw_selfAnxious',\n",
    " 'PostDiscussionGeneral_raw_selfInsight',\n",
    " 'PostDiscussionGeneral_raw_selfLearned',\n",
    " 'PostDiscussionGeneral_raw_selfSpeakUp',\n",
    " 'PostDiscussionGeneral_raw_selfVoice',\n",
    " 'PostDiscussionPerceptionOfOthers_raw_defensive',\n",
    " 'PostDiscussionPerceptionOfOthers_raw_liking',\n",
    " 'PostDiscussionPerceptionOfOthers_raw_partnerListening',\n",
    " 'PostDiscussionPerceptionOfOthers_raw_partnerPerspectiveTaking',\n",
    " 'PostDiscussionPerceptionOfOthers_raw_partnerTalking',\n",
    " 'PostDiscussionPerceptionOfOthers_raw_polite',\n",
    " 'PostDiscussionPerceptionOfOthers_raw_selfPerspectiveTaking',\n",
    " 'deal_sheet_agreement_binary',\n",
    " 'post_negotiation_conflict_1',\n",
    " 'post_negotiation_conflict_2',\n",
    " 'post_negotiation_conflict_3',\n",
    " 'post_negotiation_conflict_4',\n",
    " 'post_negotiation_decision_leadership_1',\n",
    " 'post_negotiation_decision_leadership_2',\n",
    " 'post_negotiation_goal_similarity',\n",
    " 'post_negotiation_viability_1',\n",
    " 'post_negotiation_viability_2',\n",
    " 'role']]\n",
    "\n",
    "\n",
    "df.columns = ['groupId',\n",
    " 'playerId',\n",
    " 'discussionDepth',\n",
    " 'discussionDisagreement',\n",
    " 'discussionEnjoy',\n",
    " 'discussionTension',\n",
    " 'selfAnxious',\n",
    " 'selfInsight',\n",
    " 'selfLearned',\n",
    " 'selfSpeakUp',\n",
    " 'selfVoice',\n",
    " 'defensive',\n",
    " 'liking',\n",
    " 'partnerListening',\n",
    " 'partnerPerspectiveTaking',\n",
    " 'partnerTalking',\n",
    " 'polite',\n",
    " 'selfPerspectiveTaking',\n",
    " 'agreement_binary',\n",
    " 'conflict_1',\n",
    " 'conflict_2',\n",
    " 'conflict_3',\n",
    " 'conflict_4',\n",
    " 'decision_leadership_1',\n",
    " 'decision_leadership_2',\n",
    " 'goal_similarity',\n",
    " 'viability_1',\n",
    " 'viability_2',\n",
    " 'role']\n",
    "\n",
    "\n",
    "\n",
    "df['agreement_binary'] = df['agreement_binary'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "\n",
    "list(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_principal_components(df, variables, max_components=None):\n",
    "    # Drop rows with NaN values\n",
    "    df = df.dropna(subset=variables)\n",
    "    \n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    standardized_data = scaler.fit_transform(df[variables])\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = PCA()\n",
    "    pca.fit(standardized_data)\n",
    "    \n",
    "    # Determine the number of components to retain (explaining 95% variance or max_components)\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "    num_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "    if max_components:\n",
    "        num_components = min(num_components, max_components)\n",
    "    \n",
    "    # Apply PCA with selected components\n",
    "    pca = PCA(n_components=num_components)\n",
    "    reduced_data = pca.fit_transform(standardized_data)\n",
    "    \n",
    "    # Identify variable groupings based on factor loadings\n",
    "    loadings = pd.DataFrame(pca.components_.T, index=variables, columns=[f'PC{i+1}' for i in range(num_components)])\n",
    "    print(\"PCA Factor Loadings:\\n\", loadings)\n",
    "    \n",
    "    # Display explained variance for each component and cumulative variance\n",
    "    for i in range(min(3, len(explained_variance_ratio))):\n",
    "        print(f\"PC{i+1} explains: {explained_variance_ratio[i] * 100:.2f}%\")\n",
    "    print(f\"Cumulative variance explained by the first {num_components} PCs: {cumulative_variance[num_components-1] * 100:.2f}%\")\n",
    "    \n",
    "    return reduced_data, loadings, df, explained_variance_ratio, cumulative_variance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_kmeans_clustering(reduced_data, df, variables, max_clusters=10, n_bootstrap=100, custom_colors=None):\n",
    "    metrics_summary = []\n",
    "    optimal_ks = []\n",
    "    K = range(2, max_clusters + 1)\n",
    "\n",
    "    # Evaluate clustering for different numbers of clusters\n",
    "    for k in K:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(reduced_data)\n",
    "\n",
    "        silhouette = silhouette_score(reduced_data, labels)\n",
    "        davies_bouldin = davies_bouldin_score(reduced_data, labels)\n",
    "        calinski_harabasz = calinski_harabasz_score(reduced_data, labels)\n",
    "        distortion = np.sum(np.min(cdist(reduced_data, kmeans.cluster_centers_, 'euclidean'), axis=1))\n",
    "\n",
    "        metrics_summary.append({\n",
    "            'k': k,\n",
    "            'Silhouette Score': silhouette,\n",
    "            'Davies-Bouldin Index': davies_bouldin,\n",
    "            'Calinski-Harabasz Index': calinski_harabasz,\n",
    "            'Distortion': distortion\n",
    "        })\n",
    "\n",
    "    # Determine optimal k for each metric\n",
    "    optimal_k_silhouette = max(metrics_summary, key=lambda x: x['Silhouette Score'])['k']\n",
    "    optimal_k_davies_bouldin = min(metrics_summary, key=lambda x: x['Davies-Bouldin Index'])['k']\n",
    "    optimal_k_calinski_harabasz = max(metrics_summary, key=lambda x: x['Calinski-Harabasz Index'])['k']\n",
    "    optimal_k_distortion = min(metrics_summary, key=lambda x: x['Distortion'])['k']\n",
    "\n",
    "    # Average the suggested k values\n",
    "    optimal_k = int(np.mean([optimal_k_silhouette, optimal_k_davies_bouldin, optimal_k_calinski_harabasz, optimal_k_distortion]))\n",
    "    print(f\"Optimal number of clusters based on averaged metrics: {optimal_k}\")\n",
    "\n",
    "    # Final clustering\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(reduced_data)\n",
    "\n",
    "    # Map cluster labels to descriptive names\n",
    "    cluster_names = {\n",
    "        0: \"Open\\nCollaboration\",\n",
    "        1: \"Unconstructive\\nDisagreement\",\n",
    "        2: \"Constructive\\nDisagreement\"\n",
    "    }\n",
    "    df['Cluster'] = [cluster_names.get(label, f\"Cluster {label}\") for label in clusters]\n",
    "\n",
    "    # Cluster Stability Analysis using Bootstrapping\n",
    "    ari_scores = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        bootstrap_sample = resample(reduced_data)\n",
    "        kmeans_bootstrap = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "        clusters_bootstrap = kmeans_bootstrap.fit_predict(bootstrap_sample)\n",
    "        ari = adjusted_rand_score(clusters, clusters_bootstrap[:len(clusters)])\n",
    "        ari_scores.append(ari)\n",
    "    print(f\"Average Adjusted Rand Index (Cluster Stability): {np.mean(ari_scores):.2f}\")\n",
    "\n",
    "    # Feature Importance with XGBoost\n",
    "    model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "    model.fit(df[variables], clusters)\n",
    "\n",
    "    # Cross-validation with XGBoost\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(model, df[variables], clusters, cv=cv)\n",
    "    print(f\"XGBoost Cross-Validation Accuracy: {np.mean(cv_scores):.2f} ± {np.std(cv_scores):.2f}\")\n",
    "\n",
    "    # Identify Top 5 Features with SHAP Scores for Each Cluster\n",
    "    shap_values = shap.Explainer(model)(df[variables])\n",
    "\n",
    "    # Handle multiclass SHAP outputs\n",
    "    if len(shap_values.values.shape) == 3:\n",
    "        predicted_classes = model.predict(df[variables])\n",
    "        indices = np.arange(shap_values.values.shape[0])\n",
    "        shap_values = shap_values.values[indices, :, predicted_classes]\n",
    "    else:\n",
    "        shap_values = shap_values.values\n",
    "\n",
    "    feature_importance = pd.DataFrame(\n",
    "        shap_values, columns=df[variables].columns, index=df.index  # Preserve sign of SHAP values\n",
    "    )\n",
    "\n",
    "    top_features_per_cluster = []\n",
    "    for cluster_label, cluster_name in cluster_names.items():\n",
    "        cluster_indices = df['Cluster'] == cluster_name\n",
    "        cluster_feature_importance = feature_importance.loc[cluster_indices].mean().sort_values(ascending=False)\n",
    "        top_features = cluster_feature_importance.head(5)\n",
    "\n",
    "        # Calculate average SHAP values (with directionality)\n",
    "        avg_shap_values = feature_importance.loc[cluster_indices].mean()\n",
    "\n",
    "        top_features_per_cluster.append({\n",
    "            'Cluster': cluster_name,\n",
    "            'Top Features': ', '.join(top_features.index),\n",
    "            'Positive SHAP Scores': ', '.join(f\"{score:.3f}\" for score in top_features[top_features > 0].values),\n",
    "            'Negative SHAP Scores': ', '.join(f\"{score:.3f}\" for score in top_features[top_features < 0].values)\n",
    "        })\n",
    "\n",
    "    # Cluster Visualization\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_results = tsne.fit_transform(reduced_data)\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "     # Apply custom colors if provided\n",
    "    if custom_colors is None:\n",
    "        custom_colors = {\n",
    "            \"Open\\nCollaboration\": \"#0000cd\",\n",
    "            \"Unconstructive\\nDisagreement\": \"#008080\",\n",
    "            \"Constructive\\nDisagreement\": \"#c71585\"\n",
    "        }\n",
    "        \n",
    "    scatter = sns.scatterplot(\n",
    "        x=tsne_results[:, 0],\n",
    "        y=tsne_results[:, 1],\n",
    "        hue=df['Cluster'],\n",
    "        palette=custom_colors,  # Modern color palette\n",
    "        s=100,                # Slightly smaller dots for aesthetics\n",
    "        alpha=0.80,          # Slight transparency\n",
    "        edgecolor='white',   # White border around points\n",
    "        linewidth=0.8        # Border thickness\n",
    "    )\n",
    "    plt.title('Negotiation Clusters', fontsize=20)\n",
    "    plt.xlabel('t-SNE Component 1', fontsize=18)\n",
    "    plt.ylabel('t-SNE Component 2', fontsize=18)\n",
    "    plt.legend(title='', fontsize=18)\n",
    "    plt.grid(False)  # Remove background grid\n",
    "\n",
    "    # Save the clustering plot\n",
    "    plt.savefig('cluster_visualization.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # Display and Save Top Features Table with SHAP Scores\n",
    "    top_features_df = pd.DataFrame(top_features_per_cluster)\n",
    "    top_features_df.to_csv('top_features_by_cluster.csv', index=False)\n",
    "    print(top_features_df)\n",
    "\n",
    "    return df, metrics_summary, np.mean(ari_scores), np.mean(cv_scores), np.std(cv_scores), top_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 2, 5, 1])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df['discussionTension'] = pd.to_numeric(df['discussionTension'].replace('1S', 1))\n",
    "df['discussionTension'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_num = df.drop(['role', 'groupId', 'playerId'], axis=1)\n",
    "\n",
    "reduced_data, loadings, df_pca, explained_variance_ratio, cumulative_variance = identify_principal_components(df_num, list(df_num), max_components=50)\n",
    "print(explained_variance_ratio)\n",
    "print(cumulative_variance)\n",
    "\n",
    "loadings.to_csv('/Output/Clustering/pca_loadings.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = perform_kmeans_clustering(reduced_data, df_pca, max_clusters=5)\n",
    "\n",
    "df, metrics_summary, ari_score, cv_score, cv_scores, top_features = perform_kmeans_clustering(reduced_data, df_pca, list(df_num), max_clusters=7, n_bootstrap=100)\n",
    "print(list(top_features['Top Features']))\n",
    "df.to_csv('Output/Clustering/cluster_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmenting conversations by conflict cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4774, 16)\n",
      "(90, 27)\n"
     ]
    }
   ],
   "source": [
    "df_text = pd.read_csv('./Output/super_May22/Text_agg.csv')\n",
    "list(df_text)\n",
    "print(df_text.shape)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_conflicts = df.copy()\n",
    "group_conflicts['groupId'] = df_raw['groupId']\n",
    "\n",
    "\n",
    "# Extract the last 6 characters from PairID and groupId\n",
    "df_text['MergeKey'] = df_text['PairID'].str[-6:]\n",
    "group_conflicts['MergeKey'] = group_conflicts['groupId'].str[-6:]\n",
    "\n",
    "# Perform a left join\n",
    "merged_data = df_text.merge(group_conflicts, on='MergeKey', how='left')\n",
    "\n",
    "list(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "merged_data[['word_count_scaled', 'Sentiment_scaled', 'Contested_scaled']] = scaler.fit_transform(\n",
    "    merged_data[['word_count', 'Sentiment', 'Contested']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a Left-to-Right transition matrix\n",
    "def left_to_right_transition_matrix(n_states):\n",
    "    \"\"\"Create a Left-to-Right transition matrix for HMM.\"\"\"\n",
    "    transition_matrix = np.zeros((n_states, n_states))\n",
    "    for i in range(n_states):\n",
    "        if i < n_states - 1:\n",
    "            transition_matrix[i, i] = 0.7  # Self-transition probability\n",
    "            transition_matrix[i, i + 1] = 0.3  # Forward transition probability\n",
    "        else:\n",
    "            transition_matrix[i, i] = 1.0  # Last state stays in itself\n",
    "    return transition_matrix\n",
    "\n",
    "# Function to enforce varying number of stages using BIC\n",
    "def find_optimal_hmm(features, min_states=2, max_states=5):\n",
    "    best_bic = float('inf')\n",
    "    best_model = None\n",
    "    best_num_states = None\n",
    "    for n_states in range(min_states, max_states + 1):\n",
    "        try:\n",
    "            # Initialize HMM with Left-to-Right constraints\n",
    "            hmm_model = GaussianHMM(n_components=n_states, covariance_type=\"diag\", random_state=42)\n",
    "            hmm_model.startprob_ = np.zeros(n_states)\n",
    "            hmm_model.startprob_[0] = 1.0  # Start always from the first state\n",
    "            hmm_model.transmat_ = left_to_right_transition_matrix(n_states)\n",
    "            hmm_model.fit(features)\n",
    "            \n",
    "            # Compute BIC\n",
    "            n_params = (\n",
    "                n_states ** 2 +  # Transition probabilities\n",
    "                n_states * features.shape[1] * 2 +  # Means and variances\n",
    "                n_states  # Initial probabilities\n",
    "            )\n",
    "            log_likelihood = hmm_model.score(features)\n",
    "            bic = -2 * log_likelihood + n_params * np.log(features.shape[0])\n",
    "            \n",
    "            if bic < best_bic:\n",
    "                best_bic = bic\n",
    "                best_model = hmm_model\n",
    "                best_num_states = n_states\n",
    "        except Exception:\n",
    "            # Ignore convergence errors\n",
    "            continue\n",
    "    return best_model, best_num_states\n",
    "\n",
    "# Enforce sequential order for StageCluster\n",
    "def enforce_sequential_order(df, cluster_col='StageCluster'):\n",
    "    \"\"\"\n",
    "    Enforce sequential order for StageCluster within each conversation (PairID).\n",
    "    Adjusts StageCluster so it only progresses forward or stays the same.\n",
    "    \"\"\"\n",
    "    corrected_clusters = []\n",
    "    \n",
    "    for pair_id, group in df.groupby('PairID'):\n",
    "        current_max = -1  # Track the maximum StageCluster seen so far\n",
    "        corrected = []\n",
    "\n",
    "        for cluster in group[cluster_col]:\n",
    "            if cluster >= current_max:\n",
    "                current_max = cluster  # Allow progression or staying the same\n",
    "            corrected.append(current_max)  # Enforce monotonic progression\n",
    "        \n",
    "        corrected_clusters.extend(corrected)\n",
    "    \n",
    "    df[cluster_col] = corrected_clusters\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment each conversation\n",
    "stages = []\n",
    "for pair_id, group in merged_data.groupby('PairID'):\n",
    "    # Extract features\n",
    "    features = group[['word_count_scaled', 'Sentiment_scaled', 'Contested_scaled']].values\n",
    "\n",
    "    # Find the optimal HMM for this conversation\n",
    "    hmm_model, n_states = find_optimal_hmm(features)\n",
    "\n",
    "    if hmm_model:\n",
    "        # Predict stages with the best HMM\n",
    "        group['StageCluster'] = hmm_model.predict(features)\n",
    "\n",
    "        # Map clusters to generic stage names\n",
    "        group['Stage'] = group['StageCluster'].map(lambda x: f\"Stage {x + 1}\")\n",
    "    else:\n",
    "        group['StageCluster'] = -1\n",
    "        group['Stage'] = \"Unclassified\"\n",
    "\n",
    "    stages.append(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine processed conversations\n",
    "data_segmented = pd.concat(stages, ignore_index=True)\n",
    "\n",
    "# Enforce sequential order on StageCluster\n",
    "data_segmented = enforce_sequential_order(data_segmented, cluster_col='StageCluster')\n",
    "\n",
    "# Update Stage labels based on corrected clusters\n",
    "data_segmented['Stage'] = data_segmented['StageCluster'].map(lambda x: f\"Stage {x + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(data_segmented))\n",
    "# Summarize conversation stages for each conflict group\n",
    "summary = (\n",
    "    data_segmented.groupby(['Cluster', 'StageCluster'])\n",
    "    .agg(\n",
    "        avg_turns=('Turn', lambda x: len(x.unique())),  # Average count of unique turns\n",
    "        std_turns=('Turn', lambda x: np.std([len(list(g)) for _, g in x.groupby(x)])),\n",
    "        # avg_wordcount=('word_count', lambda x: x.sum() / len(x)),  # Average wordcount by turn\n",
    "        # std_wordcount=('word_count', lambda x: np.sqrt(np.mean((x - x.sum() / len(x))**2))),\n",
    "        sum_wordcount=('word_count', 'sum'),\n",
    "        # std_wordcount=('word_count', 'std'),\n",
    "        avg_sentiment=('Sentiment', 'mean'),\n",
    "        std_sentiment=('Sentiment', 'std'),\n",
    "        avg_contested=('Contested', 'mean'),\n",
    "        std_contested=('Contested', 'std'),\n",
    "        typical_start_time=('Start Time', 'mean'),\n",
    "        typical_end_time=('End Time', 'mean')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "summary['avg_wordcount'] = summary['sum_wordcount'] / summary['avg_turns']\n",
    "# summary['std_wordcount'] = np.sqrt(summary['sum_wordcount'] / summary['avg_turns'])\n",
    "summary['avg_duration'] = summary['typical_end_time'] - summary['typical_start_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the summary\n",
    "data_segmented.to_csv(\"./Output/Clustering/Segmented_Conversations_With_Conflicts.csv\", index=False)\n",
    "summary.to_csv(\"./Output/Clustering/Conflict_Group_Stage_Summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
