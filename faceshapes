import pandas as pd
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
import cv2

from mediapipe import solutions
from mediapipe.framework.formats import landmark_pb2
import numpy as np

from deepface import DeepFace

cap = cv2.VideoCapture("test_video.mov")
fps = cap.get(cv2.CAP_PROP_FPS)

print(cap, fps)

BaseOptions = mp.tasks.BaseOptions
VisionRunningMode = mp.tasks.vision.RunningMode

def face_cue_detector(mp_image):

    FaceLandmarker = mp.tasks.vision.FaceLandmarker
    FaceLandmarkerOptions = mp.tasks.vision.FaceLandmarkerOptions

    base_options = python.BaseOptions(model_asset_path='face_landmarker_v2_with_blendshapes.task')
    options = vision.FaceLandmarkerOptions(base_options=base_options,
                                        output_face_blendshapes=True,
                                        output_facial_transformation_matrixes=True,
                                        running_mode=VisionRunningMode.IMAGE,
                                        num_faces = 2)

    
    detector = vision.FaceLandmarker.create_from_options(options)
    
    face_cue_detection_result = detector.detect(mp_image)
    
    face_blendshapes = face_cue_detection_result.face_blendshapes[0]
    names = [face_blendshapes_category.category_name for face_blendshapes_category in face_blendshapes]
    scores = [face_blendshapes_category.score for face_blendshapes_category in face_blendshapes]
    
    return names, scores


def hand_gesture_detector(mp_image):

    GestureRecognizer = mp.tasks.vision.GestureRecognizer
    GestureRecognizerOptions = mp.tasks.vision.GestureRecognizerOptions

    options = GestureRecognizerOptions(
        base_options=BaseOptions(model_asset_path='gesture_recognizer.task'),
        running_mode=VisionRunningMode.IMAGE)
    GestureRecognizer.create_from_options(options)
    
    detector = GestureRecognizer.create_from_options(options)
    
    gestures = detector.recognize(mp_image).GestureRecognizerResult

    return gestures

def emotion_detector(frame):
    emotions = DeepFace.analyze(frame, actions = ['emotion'], enforce_detection= False)
    
    return emotions[0]['emotion']

results = []
timestamps = []
face_blendshapes_results = []
hand_gesture_results = []
topemotion_results = []
while True:

    ret, frame = cap.read()
    
    if not ret:
        break
    
    timestamp_ms = cap.get(cv2.CAP_PROP_POS_MSEC)
    timestamps.append(timestamp_ms)
    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)
    
    # hand_gestures = hand_gesture_detector(mp_image)
    # hand_gesture_results.append(hand_gestures)
    
    face_blendshapes_names, face_blendshapes_scores = face_cue_detector(mp_image)
    face_blendshapes_results.append(face_blendshapes_scores)

    # dominant_emotion = emotion_detector(frame)
    # print(dominant_emotion)

print(face_blendshapes_results)
cap.release()

